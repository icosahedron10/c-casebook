DEMO_SCRIPTS.txt — Casebook Repeatable Demos
Version: v1

Purpose
These demo scripts are short, repeatable checks that your project works.
They are designed to be:
- fast (1–3 minutes)
- manual-friendly
- deterministic
- aligned to the “minimum viable” requirements of each week part

How to store demo artifacts (recommended)
Inside each project folder, create:
- tests/
    demo_input_weekA.txt   (optional; scripted stdin)
    demo_input_weekB.txt   (optional)
    expected_notes.txt     (what output lines to confirm)

This is optional. You can also run demos fully interactively.

General construction rules
- Keep demo inputs short.
- Prefer integer cents for money.
- Prefer “append order” listings (no sorting required unless specified).
- Verify invariants, not perfection.

Reset rules (Projects 3+)
- If your project uses data/runtime/:
  delete data/runtime/clients.txt and data/runtime/transactions.txt
- If you use a simpler data/*.txt layout:
  delete the runtime files in that location

Fixture rule (Projects 3+)
- Fixtures are known-good datasets under data/fixtures/.
- You may copy fixtures into runtime to start from a predictable state.

----------------------------------------------------------------
PROJECT 1 — Demo Scripts (Weeks 1–2)

P1A (Week 1) Demo: “Menu loop never breaks”
Build
- gcc ... (per Project 1 spec)

Run + verify
1) Start program.
2) Enter an invalid choice (letters): “abc”
   Expect: an error message and return to menu.
3) Enter a blank line (just press Enter).
   Expect: error + return to menu.
4) Enter a large number (e.g., 999).
   Expect: error + return to menu.
5) Enter Quit.
   Expect: clean exit (no crash, no infinite loop).

PASS if
- program never crashes
- always returns to menu after invalid input
- quit exits cleanly

P1B (Week 2) Demo: “Add client confirmation + predictable list”
Run + verify
1) Add client:
   - name: Alice
   - contact: alice@example.com
   Expect: “Added client #1 …”
2) List clients:
   Expect: either shows Alice (if you stored one client) OR explains no storage (allowed option).
3) Add another client:
   - name: Bob
   - contact: bob@example.com
   Expect: “Added client #2 …”
4) List clients again:
   Expect: deterministic behavior (either last client only OR message, per your v0 choice).
5) Quit cleanly.

PASS if
- IDs increment only on successful add
- no “input skipping” between prompts

----------------------------------------------------------------
PROJECT 2 — Demo Scripts (Weeks 3–4)

P2A (Week 3) Demo: “Clients: add + list; transactions stub OK”
Run + verify
1) Add 2 clients:
   - Alice / alice@example.com
   - Bob / bob@example.com
2) List clients:
   Expect: two lines (or a clear structured print) showing IDs 1 and 2.
3) Trigger each menu option at least once.
4) Quit.

PASS if
- list matches what you added
- menu stays stable
- main() remains small (code review check, not runtime)

P2B (Week 4) Demo: “Transactions require existing client_id”
Run + verify
1) Add 1 client (Alice).
2) Add transaction for client_id 1:
   - amount: 1200
   - memo: lunch
   Expect: success + txn_id #1
3) Add transaction for client_id 999:
   Expect: reject; txn_id is NOT consumed
4) List transactions:
   Expect: exactly one transaction (the successful one)
5) Quit.

PASS if
- referential integrity is enforced
- txn IDs are monotonic and not consumed on failure

----------------------------------------------------------------
PROJECT 3 — Demo Scripts (Weeks 5–6)

P3A (Week 5) Demo: “Struct migration unchanged behavior”
Run + verify
1) Add 2 clients; list them.
2) Add 2 transactions for valid clients; list them.
3) Attempt invalid transaction client_id; expect reject + no ID consumed.
4) Quit.

PASS if
- behavior matches Project 2, but internal storage is now structs

P3B (Week 6) Demo: “Save on exit, load on start”
Setup
- Reset runtime files (delete them).
- Optional: copy fixtures into runtime first.

Run + verify
1) Start program (should load empty dataset if runtime missing).
2) Add 1 client and 1 transaction.
3) Quit (should save).
4) Restart program.
5) List clients and transactions.
   Expect: the same items appear after restart.

PASS if
- missing files are treated as empty (no crash)
- save + load round-trip works

----------------------------------------------------------------
PROJECT 4 — Demo Scripts (Weeks 7–8)

P4A (Week 7) Demo: “Prep refactor didn’t break behavior”
Run + verify
- Repeat P3B demo quickly (add, save, restart, load).
PASS if
- behavior unchanged

P4B (Week 8) Demo: “Dynamic growth works”
Setup
- Reset runtime.
Run + verify
1) Add enough clients to exceed your old fixed limit (or at least > initial capacity).
   Example: add 10 clients when initial capacity is 4.
2) List clients; confirm they all exist.
3) Add enough transactions to force growth as well.
4) Quit, restart, load, list.
PASS if
- growth happens without crashes
- count/capacity logic stays correct
- save/load still works

Optional (recommended)
- Build with ASan and run the same demo.

----------------------------------------------------------------
PROJECT 5 — Demo Scripts (Weeks 9–10)

P5A (Week 9) Demo: “Type audit didn’t break behavior”
Run + verify
- Repeat a short P4B-style run (add, list, save, restart).
PASS if
- no warnings in build
- normal flows still work

P5B (Week 10) Demo: “Enum + flags + exactly two reports + persistence”
Setup
- Reset runtime.
Run + verify
1) Add 1 client.
2) Toggle FLAGGED for that client (or your chosen flag action).
3) Add 3 transactions for that client, one per type:
   - RETAINER
   - EXPENSE
   - PAYMENT
4) List transactions:
   Expect: type strings appear (not raw numbers only).
5) Run both reports you implemented:
   Expect: deterministic output; totals make sense.
6) Quit, restart, rerun at least one report:
   Expect: flags and types persisted.

PASS if
- exactly two reports exist and run
- types/flags persist
- ID invariants remain true

----------------------------------------------------------------
PROJECT 6 — Demo Scripts (Weeks 11–12)

P6A (Week 11) Demo: “Multifile refactor: same behavior”
Build
- make
Run + verify
- Repeat P5B demo quickly (shortened if needed).
PASS if
- outputs match prior behavior
- no warnings
- no architecture cheating (no shared globals across modules)

P6B (Week 12) Demo: “Make targets + config hook”
Build
- make debug
- make asan
Run + verify
1) Run with defaults; behavior unchanged.
2) Run with your chosen configuration hook:
   - CLI: ./casebook --data-dir <path>
   - OR env var: CASEBOOK_DATA_DIR=... ./casebook
3) Confirm it uses the chosen runtime location (best verified by saving files there).
PASS if
- targets build
- debug logging only appears when enabled
- config hook works without crashing

----------------------------------------------------------------
PROJECT 7 — Demo Scripts (Weeks 13–14)

P7A (Week 13) Demo: “Four reports + deterministic output”
Setup
- Copy fixtures to runtime (recommended).
Run + verify
1) Run all four reports.
2) Run them again in the same session.
   Expect: identical output each time.
3) Restart program and run reports again.
   Expect: identical output for the same dataset.
PASS if
- exactly four reports exist
- outputs are stable and documented

P7B (Week 14) Demo: “Stability abuse test”
Setup
- Start from empty runtime dataset (delete runtime files).
Run + verify
1) Run reports with empty dataset.
   Expect: clear “no data” messages, no crash.
2) Enter invalid menu input repeatedly (letters, blank lines, huge numbers).
   Expect: no crash, returns to menu.
3) Try invalid IDs (client_id that doesn’t exist).
   Expect: rejects cleanly; no ID consumed.
4) Quit, restart, load.
PASS if
- make asan run produces no sanitizer complaints in normal and “abuse” paths
- program never crashes or corrupts state

----------------------------------------------------------------
How to extend demos (if you want slightly more rigor without a framework)
- Add a “scripted stdin” demo_input file for a week.
- Capture output to tests/actual_output.txt.
- Keep a short checklist in tests/expected_notes.txt:
  - “Should contain: Added client #1”
  - “Totals must match: …”
This is enough to catch regressions without building a test system.
